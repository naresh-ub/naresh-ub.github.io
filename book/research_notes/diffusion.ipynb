{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ca9ffa-ef07-44a4-b742-09bd0df6b2bc",
   "metadata": {},
   "source": [
    "# Understanding Diffusion from scratch\n",
    "\n",
    "```{warning}\n",
    "This notes is a work in progress, the content is not organized yet, only the content is dumped. Once entire content is complete, I will work on organizing the web page for readability.\n",
    "```\n",
    "\n",
    "There are two parts to this tutorial. The first part uses the [Step-by-Step Diffusion](https://arxiv.org/pdf/2406.08929v1) tutorial for strong foundations on the topic, the second part uses the [Stanley Chan's Tutorial on Diffusion ModelsTutorial on Diffusion Models](https://arxiv.org/pdf/2403.18103) for clear intuition on Stochastic Differential Equation formulation in diffusion models.\n",
    "\n",
    "## Fundamentals\n",
    "\n",
    "Each fundamental is clearly explained. The content for fundamentals is intentionally verbose.\n",
    "\n",
    "```{admonition} What is a random variable?\n",
    ":class: note, dropdown\n",
    "\n",
    "A **random variable** is a way to assign numbers to the outcomes of a random process.\n",
    "\n",
    "**Think of it like this:**  \n",
    "- You roll a die. The result (1, 2, 3, 4, 5, or 6) is a **random number** → This is a **random variable**.  \n",
    "- You measure the time it takes for a website to load. The time is **random and can take any real value** → Another **random variable**.  \n",
    "\n",
    "---\n",
    "\n",
    "**Types of Random Variables**  \n",
    "\n",
    "**1. Discrete Random Variable**  \n",
    "A discrete random variable can take only specific values (like whole numbers).  \n",
    "\n",
    "**Example: Rolling a Fair Die**  \n",
    "Let $ X $ be the number shown on a fair six-sided die. The possible values of $ X $ are:  \n",
    "\n",
    "$$\n",
    "X \\in \\{1, 2, 3, 4, 5, 6\\}\n",
    "$$\n",
    "\n",
    "Since the die is fair, each outcome has an equal probability:\n",
    "\n",
    "$$\n",
    "P(X = x) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{6}, & x = 1,2,3,4,5,6 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For example, the probability of rolling a **4** is:\n",
    "\n",
    "$$\n",
    "P(X = 4) = \\frac{1}{6}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**2. Continuous Random Variable**  \n",
    "A continuous random variable can take **any value within a range**.  \n",
    "\n",
    "**Example: Webpage Loading Time**  \n",
    "Let $ Y $ be the time (in seconds) for a webpage to load. The possible values of $ Y $ are:  \n",
    "\n",
    "$$\n",
    "Y \\in [0, \\infty)\n",
    "$$\n",
    "\n",
    "Since $ Y $ can take infinitely many values, we use a **probability density function (PDF)** instead of exact probabilities.  \n",
    "\n",
    "If $ Y $ follows an **exponential distribution**, its PDF is:\n",
    "\n",
    "$$\n",
    "f_Y(y) = \\lambda e^{-\\lambda y}, \\quad y \\geq 0\n",
    "$$\n",
    "\n",
    "where $ \\lambda $ is a constant that controls the rate of decay.  \n",
    "\n",
    "To find the probability that the webpage loads in **less than 3 seconds**, we integrate the PDF:\n",
    "\n",
    "$$\n",
    "P(Y \\leq 3) = \\int_0^3 \\lambda e^{-\\lambda y} dy\n",
    "$$\n",
    "\n",
    "This gives the probability that the page loads within 3 seconds.  \n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**  \n",
    "- **Discrete Random Variable** → Takes countable values (e.g., die rolls, number of heads in coin flips).  \n",
    "- **Continuous Random Variable** → Takes any value in a range (e.g., time, temperature, height). \n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} What is a Probability Distribution?\n",
    ":class: note, dropdown\n",
    "\n",
    "A **probability distribution** describes how values of a random variable are distributed. It tells us the likelihood of different outcomes occurring.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Discrete Probability Distribution**  \n",
    "A discrete probability distribution is used for **discrete random variables**, where the variable takes a finite or countably infinite number of values.\n",
    "\n",
    "Each possible value $ x_i $ has an associated probability $ P(X = x_i) $, and the total probability must sum to 1:\n",
    "\n",
    "$$\n",
    "\\sum_{i} P(X = x_i) = 1\n",
    "$$\n",
    "\n",
    "**Probability Mass Function (PMF):**  \n",
    "For a discrete random variable, the **probability mass function (PMF)**, denoted as $ P(X = x) $, gives the probability of the random variable taking a specific value $ x $. It satisfies:\n",
    "\n",
    "1. $ 0 \\leq P(X = x) \\leq 1 $ for all $ x $.\n",
    "2. The total probability is 1:\n",
    "\n",
    "   $$\n",
    "   \\sum_x P(X = x) = 1\n",
    "   $$\n",
    "\n",
    "**Example: Rolling a Fair Die**  \n",
    "For a fair six-sided die, the probability of each face is:\n",
    "\n",
    "$$\n",
    "P(X = x) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{6}, & x = 1,2,3,4,5,6 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The sum of probabilities:\n",
    "\n",
    "$$\n",
    "\\sum_{x=1}^{6} P(X = x) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**2. Continuous Probability Distribution**  \n",
    "A continuous probability distribution is used for **continuous random variables**, where the variable can take any value in an interval.\n",
    "\n",
    "**Probability Density Function (PDF):**  \n",
    "Instead of a probability mass function (PMF), we use a **probability density function (PDF)**, denoted as $ f_X(x) $. The probability of the variable lying in an interval $ [a, b] $ is:\n",
    "\n",
    "$$\n",
    "P(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx\n",
    "$$\n",
    "\n",
    "For a valid probability density function, it must satisfy:\n",
    "\n",
    "1. $ f_X(x) \\geq 0 $ for all $ x $\n",
    "2. The total probability must integrate to 1:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} f_X(x) dx = 1\n",
    "$$\n",
    "\n",
    "**Example: Standard Normal Distribution (Gaussian)**  \n",
    "The normal distribution is a common continuous distribution:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}, \\quad -\\infty < x < \\infty\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\mu $ is the mean (center of the distribution).\n",
    "- $ \\sigma^2 $ is the variance (spread of the distribution).\n",
    "\n",
    "For a **standard normal distribution** ($ \\mu = 0, \\sigma^2 = 1 $):\n",
    "\n",
    "$$\n",
    "f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n",
    "$$\n",
    "\n",
    "To find the probability of $ X $ being in a range, we integrate:\n",
    "\n",
    "$$\n",
    "P(-1 \\leq X \\leq 1) = \\int_{-1}^{1} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} dx\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Key Differences Between Discrete and Continuous Distributions**  \n",
    "\n",
    "| Feature            | Discrete Distribution                 | Continuous Distribution |\n",
    "|-------------------|--------------------------------|--------------------------|\n",
    "| Random Variable Type | Takes countable values (e.g., integers) | Takes uncountable values (real numbers) |\n",
    "| Probability Function | Probability Mass Function (PMF) | Probability Density Function (PDF) |\n",
    "| Probability Calculation | $ P(X = x) $ gives exact probability | $ P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx $ |\n",
    "| Example | Rolling a die, number of heads in coin flips | Heights, weights, webpage load times |\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```{admonition} What is a Probability Density Function?\n",
    ":class: note, dropdown\n",
    "\n",
    "A **Probability Density Function (PDF)** describes the likelihood of a continuous random variable taking on a specific value. Unlike a **Probability Mass Function (PMF)** (used for discrete variables), the PDF does not give the probability of a single outcome but instead provides a function that, when integrated over an interval, gives the probability of the variable falling within that range.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**  \n",
    "For a continuous random variable $ X $, the **probability density function (PDF)**, denoted as $ f_X(x) $, satisfies the following properties:\n",
    "\n",
    "1. **Non-negativity:**  \n",
    "\n",
    "   $$\n",
    "   f_X(x) \\geq 0, \\quad \\forall x \\in \\mathbb{R}\n",
    "   $$\n",
    "\n",
    "2. **Total Probability is 1:** \n",
    "\n",
    "   $$\n",
    "   \\int_{-\\infty}^{\\infty} f_X(x) \\, dx = 1\n",
    "   $$\n",
    "\n",
    "3. **Probability of an Interval:**  \n",
    "   The probability that $ X $ lies in an interval $ [a, b] $ is given by:\n",
    "\n",
    "   $$\n",
    "   P(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx\n",
    "   $$\n",
    "\n",
    "Since a continuous random variable can take an infinite number of values, the probability of it taking a specific single value is always **zero**:\n",
    "\n",
    "$$\n",
    "P(X = x) = \\int_x^x f_X(x) \\, dx = 0\n",
    "$$\n",
    "\n",
    "This is why we always consider probabilities over intervals rather than individual points.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Uniform Distribution**  \n",
    "A continuous random variable $ X $ following a **Uniform Distribution** over the interval $ [a, b] $ has a PDF:\n",
    "\n",
    "$$\n",
    "f_X(x) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{b - a}, & a \\leq x \\leq b \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The probability of $ X $ being in a subinterval $ [c, d] $ (where $ a \\leq c < d \\leq b $) is:\n",
    "\n",
    "$$\n",
    "P(c \\leq X \\leq d) = \\int_c^d \\frac{1}{b - a} \\, dx = \\frac{d - c}{b - a}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Normal (Gaussian) Distribution**  \n",
    "A **Normal (Gaussian) Distribution** is given by the PDF:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}, \\quad -\\infty < x < \\infty\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\mu $ is the **mean** (center of the distribution).\n",
    "- $ \\sigma^2 $ is the **variance** (spread of the distribution).\n",
    "\n",
    "To find the probability of $ X $ falling in a certain range $ [a, b] $, we compute:\n",
    "\n",
    "$$\n",
    "P(a \\leq X \\leq b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} dx\n",
    "$$\n",
    "\n",
    "Since this integral **does not have a closed-form solution**, we use numerical integration or lookup tables for cumulative probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Differences Between PMF and PDF**  \n",
    "\n",
    "| Feature            | PMF (Discrete)                          | PDF (Continuous)                    |\n",
    "|-------------------|--------------------------------|--------------------------------|\n",
    "| Definition        | $ P(X = x) $ gives exact probability of a value | $ f_X(x) $ represents density, not probability |\n",
    "| Total Probability | $ \\sum P(X = x) = 1 $ | $ \\int_{-\\infty}^{\\infty} f_X(x) dx = 1 $ |\n",
    "| Single Value Probability | $ P(X = x) > 0 $ for some $ x $ | $ P(X = x) = 0 $ for all $ x $ |\n",
    "| Example          | Number of heads in 10 coin flips | Height of people in cm |\n",
    "\n",
    "```\n",
    "```{admonition} What is a Cumulative Density Function?\n",
    ":class: note, dropdown\n",
    "The **Cumulative Distribution Function (CDF)** gives the probability that a random variable $ X $ takes on a value **less than or equal to** a given number $ x $. It is useful for describing both **discrete** and **continuous** probability distributions.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**  \n",
    "The **Cumulative Distribution Function (CDF)** of a random variable $ X $, denoted as $ F_X(x) $, is defined as:\n",
    "\n",
    "$$\n",
    "F_X(x) = P(X \\leq x)\n",
    "$$\n",
    "\n",
    "For **discrete random variables**, the CDF is the sum of probabilities up to $ x $:\n",
    "\n",
    "$$\n",
    "F_X(x) = \\sum_{t \\leq x} P(X = t)\n",
    "$$\n",
    "\n",
    "For **continuous random variables**, the CDF is obtained by integrating the probability density function (PDF):\n",
    "\n",
    "$$\n",
    "F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt\n",
    "$$\n",
    "\n",
    "where $ f_X(x) $ is the **Probability Density Function (PDF)**.\n",
    "\n",
    "---\n",
    "\n",
    "**Properties of the CDF**  \n",
    "1. **Non-decreasing Function:**  \n",
    "   Since probabilities accumulate, the CDF is always **non-decreasing**:\n",
    "\n",
    "   $$\n",
    "   F_X(a) \\leq F_X(b), \\quad \\text{for } a \\leq b\n",
    "   $$\n",
    "\n",
    "2. **Limits:**  \n",
    "   - The smallest possible value of $ X $ has a probability of **0**:\n",
    "\n",
    "     $$\n",
    "     \\lim_{x \\to -\\infty} F_X(x) = 0\n",
    "     $$\n",
    "\n",
    "   - The largest possible value of $ X $ has a probability of **1**:\n",
    "\n",
    "     $$\n",
    "     \\lim_{x \\to \\infty} F_X(x) = 1\n",
    "     $$\n",
    "\n",
    "3. **Computing Probability Between Two Values:**  \n",
    "   The probability that $ X $ lies in an interval $ [a, b] $ is:\n",
    "\n",
    "   $$\n",
    "   P(a \\leq X \\leq b) = F_X(b) - F_X(a)\n",
    "   $$\n",
    "\n",
    "4. **Relationship with PDF:**  \n",
    "   If $ X $ is continuous, the CDF and PDF are related by differentiation:\n",
    "\n",
    "   $$\n",
    "   f_X(x) = \\frac{d}{dx} F_X(x)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Discrete Random Variable (Rolling a Fair Die)**  \n",
    "Let $ X $ be the result of rolling a fair six-sided die. The **PMF** is:\n",
    "\n",
    "$$\n",
    "P(X = x) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{6}, & x = 1,2,3,4,5,6 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The **CDF** is:\n",
    "\n",
    "$$\n",
    "F_X(x) =\n",
    "\\begin{cases}\n",
    "0, & x < 1 \\\\\n",
    "\\frac{1}{6}, & 1 \\leq x < 2 \\\\\n",
    "\\frac{2}{6}, & 2 \\leq x < 3 \\\\\n",
    "\\frac{3}{6}, & 3 \\leq x < 4 \\\\\n",
    "\\frac{4}{6}, & 4 \\leq x < 5 \\\\\n",
    "\\frac{5}{6}, & 5 \\leq x < 6 \\\\\n",
    "1, & x \\geq 6\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- The probability of rolling **≤ 3** is $ F_X(3) = \\frac{3}{6} = 0.5 $.\n",
    "- The probability of rolling **between 2 and 4** is:\n",
    "\n",
    "  $$\n",
    "  P(2 \\leq X \\leq 4) = F_X(4) - F_X(2) = \\frac{4}{6} - \\frac{2}{6} = \\frac{2}{6}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Continuous Random Variable (Uniform Distribution on $ [0,1] $)**  \n",
    "For a uniform distribution between 0 and 1, the **PDF** is:\n",
    "\n",
    "$$\n",
    "f_X(x) =\n",
    "\\begin{cases}\n",
    "1, & 0 \\leq x \\leq 1 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The **CDF** is:\n",
    "\n",
    "$$\n",
    "F_X(x) =\n",
    "\\begin{cases}\n",
    "0, & x < 0 \\\\\n",
    "x, & 0 \\leq x \\leq 1 \\\\\n",
    "1, & x > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For example:\n",
    "- $ P(X \\leq 0.5) = F_X(0.5) = 0.5 $\n",
    "- $ P(0.2 \\leq X \\leq 0.8) = F_X(0.8) - F_X(0.2) = 0.8 - 0.2 = 0.6 $\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Normal (Gaussian) Distribution**  \n",
    "For a **Normal (Gaussian) distribution**:\n",
    "\n",
    "$$\n",
    "F_X(x) = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(t - \\mu)^2}{2\\sigma^2}} dt\n",
    "$$\n",
    "\n",
    "This integral **does not have a closed-form solution**, so we use numerical approximations or lookup tables.\n",
    "\n",
    "For a **standard normal distribution** ($ \\mu = 0, \\sigma^2 = 1 $), the CDF is denoted as:\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{t^2}{2}} dt\n",
    "$$\n",
    "\n",
    "Common values (from a normal table):\n",
    "- $ \\Phi(0) = 0.5 $\n",
    "- $ \\Phi(1) \\approx 0.8413 $\n",
    "- $ \\Phi(-1) \\approx 0.1587 $\n",
    "\n",
    "To find $ P(0 \\leq X \\leq 1) $ for a standard normal variable:\n",
    "\n",
    "$$\n",
    "P(0 \\leq X \\leq 1) = \\Phi(1) - \\Phi(0) = 0.8413 - 0.5 = 0.3413\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Key Differences Between PDF and CDF**  \n",
    "\n",
    "| Feature | PDF (Continuous) | CDF |\n",
    "|---------|-----------------|-----|\n",
    "| Definition | $ f_X(x) $ gives the density, not probability | $ F_X(x) = P(X \\leq x) $ gives cumulative probability |\n",
    "| Relationship | $ P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx $ | $ P(a \\leq X \\leq b) = F_X(b) - F_X(a) $ |\n",
    "| Example | $ f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} $ (Normal) | $ F_X(x) = \\int_{-\\infty}^{x} f_X(t) dt $ |\n",
    "\n",
    "```\n",
    "```{admonition} What is Expectation of a random variable?\n",
    ":class: note, dropdown\n",
    "\n",
    "The **expectation** (or **expected value**) of a random variable represents its long-term average value over many trials. It gives an idea of the **center** of the distribution.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**  \n",
    "For a random variable $ X $, the expectation (denoted as $ \\mathbb{E}[X] $) is defined as:\n",
    "\n",
    "- **For a discrete random variable:**\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\sum_{i} x_i P(X = x_i)\n",
    "  $$\n",
    "\n",
    "- **For a continuous random variable:**\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\, dx\n",
    "  $$\n",
    "\n",
    "where:\n",
    "- $ P(X = x_i) $ is the probability mass function (PMF) for discrete variables.\n",
    "- $ f_X(x) $ is the probability density function (PDF) for continuous variables.\n",
    "\n",
    "The expectation can be interpreted as a **weighted average**, where each possible value of $ X $ is weighted by its probability.\n",
    "\n",
    "---\n",
    "\n",
    "**Properties of Expectation**  \n",
    "\n",
    "1. **Linearity:**  \n",
    "   For any two random variables $ X $ and $ Y $, and constants $ a, b $:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\n",
    "   $$\n",
    "\n",
    "2. **Expectation of a Constant:**  \n",
    "   If $ c $ is a constant, then:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[c] = c\n",
    "   $$\n",
    "\n",
    "3. **Expectation of a Function of X:**  \n",
    "   If $ g(X) $ is a function of a random variable $ X $, then:\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[g(X)] = \\sum_{i} g(x_i) P(X = x_i) \\quad \\text{(discrete)}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\, dx \\quad \\text{(continuous)}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Discrete Random Variable (Rolling a Fair Die)**  \n",
    "Let $ X $ be the result of rolling a fair six-sided die. The possible values are $ X = \\{1,2,3,4,5,6\\} $, and the probability of each value is:\n",
    "\n",
    "$$\n",
    "P(X = x) = \\frac{1}{6}, \\quad x \\in \\{1,2,3,4,5,6\\}\n",
    "$$\n",
    "\n",
    "The expectation is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\sum_{x=1}^{6} x P(X = x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5\n",
    "$$\n",
    "\n",
    "So, if you roll a fair die many times, the **average outcome** will be **3.5**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Continuous Random Variable (Uniform Distribution on [0,1])**  \n",
    "Let $ X $ follow a uniform distribution on $ [0,1] $, meaning its PDF is:\n",
    "\n",
    "$$\n",
    "f_X(x) =\n",
    "\\begin{cases}\n",
    "1, & 0 \\leq x \\leq 1 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The expectation is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\int_0^1 x f_X(x) dx = \\int_0^1 x \\cdot 1 \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{x^2}{2} \\Big|_0^1 = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "So, the **expected value of a uniformly distributed variable** on $ [0,1] $ is **0.5**.\n",
    "\n",
    "---\n",
    "\n",
    "**Expectation and Mean in Statistics**  \n",
    "The expectation $ \\mathbb{E}[X] $ is also called the **mean** or **first moment** of a random variable and is denoted as:\n",
    "\n",
    "$$\n",
    "\\mu = \\mathbb{E}[X]\n",
    "$$\n",
    "\n",
    "For a normal distribution $ X \\sim \\mathcal{N}(\\mu, \\sigma^2) $, the expected value is simply:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\mu\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**  \n",
    "- Expectation is a **long-run average** value of a random variable.\n",
    "- For **discrete** variables, expectation is a **sum** over all possible values.\n",
    "- For **continuous** variables, expectation is an **integral** over the probability density.\n",
    "- Expectation satisfies **linearity**, meaning sums and constants can be pulled out.\n",
    "```\n",
    "```{admonition} What is meant by Expectation over a probability distribution?\n",
    ":class: tip, dropdown\n",
    "Expectation over a probability distribution means computing the **average value** of a function of a random variable, weighted by the probability distribution of that variable.\n",
    "\n",
    "In simpler terms, if a random variable $ X $ follows a certain probability distribution, the expectation tells us the **average value of $ X $** when sampled from that distribution.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**  \n",
    "For a function $ g(X) $ of a random variable $ X $, the expectation over a probability distribution is:\n",
    "\n",
    "- **For a discrete random variable** with probability mass function (PMF) $ P(X = x) $:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[g(X)] = \\sum_{x} g(x) P(X = x)\n",
    "  $$\n",
    "\n",
    "- **For a continuous random variable** with probability density function (PDF) $ f_X(x) $:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\, dx\n",
    "  $$\n",
    "\n",
    "This tells us the expected value of the function $ g(X) $ when $ X $ is sampled according to its probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "**Special Case: Expectation of $ X $ Itself**  \n",
    "If $ g(X) = X $, then the expectation simply gives the **mean** of the distribution:\n",
    "\n",
    "- **Discrete case:**\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\sum_{x} x P(X = x)\n",
    "  $$\n",
    "\n",
    "- **Continuous case:**\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\, dx\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "**Example 1: Discrete Expectation Over a Probability Distribution**  \n",
    "Consider a **biased** coin flip where $ X $ represents the number of heads in a single flip:\n",
    "\n",
    "- $ P(X = 1) = p $ (probability of heads)\n",
    "- $ P(X = 0) = 1 - p $ (probability of tails)\n",
    "\n",
    "The expectation of $ X $ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = (1 \\cdot p) + (0 \\cdot (1 - p)) = p\n",
    "$$\n",
    "\n",
    "This means that if we repeatedly flip the coin, the **average number of heads per flip** is equal to the probability of getting heads.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2: Continuous Expectation Over a Probability Distribution**  \n",
    "Let $ X $ follow a standard **normal distribution** $ \\mathcal{N}(0,1) $, meaning it has the PDF:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n",
    "$$\n",
    "\n",
    "To compute the expectation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x) dx\n",
    "$$\n",
    "\n",
    "Since the normal distribution is **symmetric around zero**, the positive and negative contributions cancel out:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = 0\n",
    "$$\n",
    "\n",
    "which makes sense because a standard normal distribution has a mean of zero.\n",
    "\n",
    "---\n",
    "\n",
    "**Expectation of a Function Over a Distribution**  \n",
    "Instead of computing $ \\mathbb{E}[X] $, we can compute the expectation of a function $ g(X) $.  \n",
    "\n",
    "For example, consider computing the expectation of $ g(X) = X^2 $ for a normal distribution:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^2] = \\int_{-\\infty}^{\\infty} x^2 f_X(x) dx\n",
    "$$\n",
    "\n",
    "For a normal distribution $ X \\sim \\mathcal{N}(\\mu, \\sigma^2) $, it is a known result that:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^2] = \\sigma^2 + \\mu^2\n",
    "$$\n",
    "\n",
    "which shows how variance and mean influence the expected squared value.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**  \n",
    "- Expectation over a probability distribution means computing the average outcome, weighted by the probability of each outcome.\n",
    "- It applies to both **discrete** (sums) and **continuous** (integrals) cases.\n",
    "- The expectation of a function $ g(X) $ can be computed using the probability distribution of $ X $.\n",
    "- The expectation of $ X $ itself gives the **mean** of the distribution.\n",
    "```\n",
    "```{admonition} Expectation of a Function Over a Distribution vs. Expectation Over a Probability Distribution\n",
    ":class: tip, dropdown\n",
    "**1. Expectation Over a Probability Distribution**  \n",
    "This refers to the general concept of computing an expected value based on a **probability distribution**. It can apply to a random variable $ X $ itself or to any function of $ X $.\n",
    "\n",
    "If $ X $ is a random variable with a probability distribution given by:\n",
    "- **PMF** $ P(X = x) $ (discrete case)\n",
    "- **PDF** $ f_X(x) $ (continuous case)\n",
    "\n",
    "Then the expectation of $ X $ itself is:\n",
    "\n",
    "- **Discrete Case:**\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\sum_{x} x P(X = x)\n",
    "  $$\n",
    "  \n",
    "- **Continuous Case:**\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x f_X(x) dx\n",
    "  $$\n",
    "\n",
    "This simply computes the **mean** or **average** value of $ X $ when sampled from its probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Expectation of a Function Over a Distribution**  \n",
    "This extends the idea of expectation to **functions of a random variable**. Instead of computing the expectation of $ X $, we compute the expectation of some function **$ g(X) $**, which could be **nonlinear**.\n",
    "\n",
    "The expectation of a function $ g(X) $ over a probability distribution is:\n",
    "\n",
    "- **Discrete Case:**\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[g(X)] = \\sum_{x} g(x) P(X = x)\n",
    "  $$\n",
    "  \n",
    "- **Continuous Case:**\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx\n",
    "  $$\n",
    "\n",
    "This formulation is useful when dealing with **moment calculations**, **variance computations**, and **statistical transformations**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Difference**  \n",
    "\n",
    "| Concept | Expectation Over a Probability Distribution | Expectation of a Function Over a Distribution |\n",
    "|---------|--------------------------------|--------------------------------|\n",
    "| Definition | Computes the expectation of the random variable itself | Computes the expectation of a function of the random variable |\n",
    "| Formula (Discrete) | $ \\mathbb{E}[X] = \\sum x P(X = x) $ | $ \\mathbb{E}[g(X)] = \\sum g(x) P(X = x) $ |\n",
    "| Formula (Continuous) | $ \\mathbb{E}[X] = \\int x f_X(x) dx $ | $ \\mathbb{E}[g(X)] = \\int g(x) f_X(x) dx $ |\n",
    "| Example | Mean of a normal distribution: $ \\mathbb{E}[X] = \\mu $ | Expected squared value: $ \\mathbb{E}[X^2] = \\sigma^2 + \\mu^2 $ |\n",
    "| Purpose | Computes the **average value** of a random variable | Computes the **average value of a transformed variable** |\n",
    "\n",
    "---\n",
    "\n",
    "**Example 1: Expectation Over a Probability Distribution (Mean of a Fair Die)**  \n",
    "Let $ X $ be the result of rolling a fair six-sided die:\n",
    "\n",
    "$$\n",
    "P(X = x) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{6}, & x \\in \\{1,2,3,4,5,6\\} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The expectation (mean value) of $ X $ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\sum_{x=1}^{6} x P(X = x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{6} (1 + 2 + 3 + 4 + 5 + 6) = 3.5\n",
    "$$\n",
    "\n",
    "This means the **average outcome of rolling the die** is **3.5**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2: Expectation of a Function Over a Distribution (Expected Squared Value of a Fair Die)**  \n",
    "Now, let's compute the expectation of $ g(X) = X^2 $:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^2] = \\sum_{x=1}^{6} x^2 P(X = x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{6} (1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{6} (1 + 4 + 9 + 16 + 25 + 36) = \\frac{91}{6} \\approx 15.17\n",
    "$$\n",
    "\n",
    "This result tells us that the **expected squared outcome** of rolling the die is **15.17**, which is **not** the same as squaring the expected value:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X]^2 = 3.5^2 = 12.25\n",
    "$$\n",
    "\n",
    "This difference is important in variance calculations:\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example 3: Expectation of a Function in a Continuous Distribution**  \n",
    "Let $ X $ follow a standard **normal distribution** $ \\mathcal{N}(0,1) $, meaning its PDF is:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n",
    "$$\n",
    "\n",
    "To compute the expectation of $ X^2 $:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^2] = \\int_{-\\infty}^{\\infty} x^2 f_X(x) dx\n",
    "$$\n",
    "\n",
    "For a normal distribution $ X \\sim \\mathcal{N}(\\mu, \\sigma^2) $, it is a known result that:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^2] = \\sigma^2 + \\mu^2\n",
    "$$\n",
    "\n",
    "For a standard normal distribution ($ \\mu = 0, \\sigma^2 = 1 $), this simplifies to:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X^2] = 1\n",
    "$$\n",
    "\n",
    "Again, this shows the difference between $ \\mathbb{E}[X] = 0 $ and $ \\mathbb{E}[X^2] = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**  \n",
    "- **Expectation over a probability distribution** finds the average of the **random variable**.\n",
    "- **Expectation of a function over a distribution** finds the average of a **transformed random variable**.\n",
    "- If $ g(X) = X $, the expectation of the function reduces to the expectation of the variable.\n",
    "- These concepts are essential for **moments**, **variance**, and **transformations** in probability.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} What is chain rule of probability?\n",
    ":class: note, dropdown\n",
    "The **chain rule of probability** (also called the **product rule**) allows us to compute the joint probability of multiple events by breaking it down into conditional probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**  \n",
    "For any $ n $ random variables $ X_1, X_2, \\dots, X_n $, their joint probability can be decomposed as:\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, \\dots, X_n) = P(X_1) P(X_2 \\mid X_1) P(X_3 \\mid X_1, X_2) \\dots P(X_n \\mid X_1, X_2, \\dots, X_{n-1})\n",
    "$$\n",
    "\n",
    "In general, for **two** random variables:\n",
    "\n",
    "$$\n",
    "P(A, B) = P(A \\mid B) P(B) = P(B \\mid A) P(A)\n",
    "$$\n",
    "\n",
    "For **three** random variables:\n",
    "\n",
    "$$\n",
    "P(A, B, C) = P(A) P(B \\mid A) P(C \\mid A, B)\n",
    "$$\n",
    "\n",
    "This process extends to **any number of variables**.\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition**  \n",
    "The chain rule breaks down the **joint probability** into a sequence of **conditional probabilities**, explaining how each variable depends on the previous ones.\n",
    "\n",
    "Example: Suppose we have three events:\n",
    "- $ X_1 $ = \"It rains\"\n",
    "- $ X_2 $ = \"I carry an umbrella\"\n",
    "- $ X_3 $ = \"I stay dry\"\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$\n",
    "P(\\text{Rain, Umbrella, Dry}) = P(\\text{Rain}) P(\\text{Umbrella} \\mid \\text{Rain}) P(\\text{Dry} \\mid \\text{Rain, Umbrella})\n",
    "$$\n",
    "\n",
    "Each probability **conditions on the previous** event, showing how they are **linked**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Probability of Drawing Cards**  \n",
    "Consider drawing **three** cards from a deck **without replacement**:\n",
    "\n",
    "- $ A $ = \"First card is an Ace\"\n",
    "- $ B $ = \"Second card is an Ace\"\n",
    "- $ C $ = \"Third card is an Ace\"\n",
    "\n",
    "The probability of drawing three Aces is:\n",
    "\n",
    "$$\n",
    "P(A, B, C) = P(A) P(B \\mid A) P(C \\mid A, B)\n",
    "$$\n",
    "\n",
    "Given there are **4 Aces in 52 cards**, we calculate:\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{4}{52}\n",
    "$$\n",
    "\n",
    "If we already drew an Ace, only **3 Aces remain in 51 cards**:\n",
    "\n",
    "$$\n",
    "P(B \\mid A) = \\frac{3}{51}\n",
    "$$\n",
    "\n",
    "If two Aces are drawn, only **2 Aces remain in 50 cards**:\n",
    "\n",
    "$$\n",
    "P(C \\mid A, B) = \\frac{2}{50}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "P(A, B, C) = \\frac{4}{52} \\times \\frac{3}{51} \\times \\frac{2}{50} = \\frac{24}{132600} \\approx 0.00018\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Application in Machine Learning and Bayesian Networks**  \n",
    "The chain rule is fundamental in:\n",
    "- **Bayesian Networks**: Used to compute probabilities in graphical models.\n",
    "- **Hidden Markov Models (HMMs)**: Used for **sequence modeling**.\n",
    "- **Naive Bayes Classifier**: Assumes conditional independence to simplify computations.\n",
    "\n",
    "For a Bayesian network with **nodes $ X_1, X_2, \\dots, X_n $** structured in a **dependency graph**, the chain rule becomes:\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, \\dots, X_n) = \\prod_{i=1}^{n} P(X_i \\mid \\text{Parents}(X_i))\n",
    "$$\n",
    "\n",
    "where **Parents($ X_i $)** are the nodes that influence $ X_i $.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**  \n",
    "- The **chain rule** expresses **joint probability** in terms of **conditional probabilities**.\n",
    "- It helps in **breaking down complex probability calculations**.\n",
    "- It is widely used in **Bayesian inference, machine learning, and probability theory**.\n",
    "```\n",
    "```{admonition} Definition of Joint Probability\n",
    ":class: note, dropdown\n",
    "\n",
    "The **joint probability** of two or more random variables is the probability that all events occur simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "**Definition**  \n",
    "For two random variables $ X $ and $ Y $, the **joint probability** is denoted as:\n",
    "\n",
    "$$\n",
    "P(X = x, Y = y)\n",
    "$$\n",
    "\n",
    "which represents the probability that **both** $ X = x $ and $ Y = y $ occur **together**.\n",
    "\n",
    "For **multiple variables** $ X_1, X_2, \\dots, X_n $, the joint probability is:\n",
    "\n",
    "$$\n",
    "P(X_1 = x_1, X_2 = x_2, \\dots, X_n = x_n)\n",
    "$$\n",
    "\n",
    "which represents the probability that all random variables take their respective values **at the same time**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Rolling Two Dice**  \n",
    "Let $ X $ and $ Y $ be the outcomes of rolling two fair six-sided dice.\n",
    "\n",
    "- There are **36 possible outcomes** since each die has 6 sides.\n",
    "- The probability of any specific outcome, e.g., $ (X = 2, Y = 5) $, is:\n",
    "\n",
    "  $$\n",
    "  P(X = 2, Y = 5) = \\frac{1}{36}\n",
    "  $$\n",
    "\n",
    "since each of the 36 outcomes is equally likely.\n",
    "\n",
    "---\n",
    "\n",
    "**Computing Joint Probability Using Conditional Probability**  \n",
    "Using the **chain rule**, joint probability can be computed as:\n",
    "\n",
    "$$\n",
    "P(X, Y) = P(X \\mid Y) P(Y)\n",
    "$$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$$\n",
    "P(X, Y) = P(Y \\mid X) P(X)\n",
    "$$\n",
    "\n",
    "This expresses the joint probability in terms of **conditional probability**.\n",
    "\n",
    "---\n",
    "\n",
    "**Independent Events and Joint Probability**  \n",
    "If $ X $ and $ Y $ are **independent**, then their joint probability simplifies to:\n",
    "\n",
    "$$\n",
    "P(X, Y) = P(X) P(Y)\n",
    "$$\n",
    "\n",
    "This means that knowing $ X $ does **not** affect the probability of $ Y $.\n",
    "\n",
    "**Example: Two Independent Coin Flips**  \n",
    "Let $ X $ and $ Y $ be the outcomes of two independent coin flips, where:\n",
    "\n",
    "- $ P(X = H) = \\frac{1}{2} $\n",
    "- $ P(Y = H) = \\frac{1}{2} $\n",
    "\n",
    "Since the flips are independent:\n",
    "\n",
    "$$\n",
    "P(X = H, Y = H) = P(X = H) P(Y = H) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Joint Probability Distribution (JPD)**  \n",
    "The **joint probability distribution** describes the probability of all possible combinations of $ X $ and $ Y $.\n",
    "\n",
    "For **discrete random variables**, the JPD is represented as a **table**.\n",
    "\n",
    "| $ X \\backslash Y $ | $ Y = 0 $ | $ Y = 1 $ |\n",
    "|------------------|---------|---------|\n",
    "| $ X = 0 $ | $ P(0,0) $ | $ P(0,1) $ |\n",
    "| $ X = 1 $ | $ P(1,0) $ | $ P(1,1) $ |\n",
    "\n",
    "Each entry in the table represents a **joint probability**.\n",
    "\n",
    "For **continuous random variables**, the JPD is defined using the **joint probability density function (PDF)**:\n",
    "\n",
    "$$\n",
    "P(a \\leq X \\leq b, c \\leq Y \\leq d) = \\int_a^b \\int_c^d f_{X,Y}(x, y) \\, dy \\, dx\n",
    "$$\n",
    "\n",
    "where $ f_{X,Y}(x, y) $ is the **joint PDF**.\n",
    "\n",
    "---\n",
    "\n",
    "**Marginal Probability from Joint Probability**  \n",
    "The **marginal probability** of a single variable is found by **summing** (discrete case) or **integrating** (continuous case) over the other variable.\n",
    "\n",
    "- **Discrete Case:**\n",
    "  \n",
    "  $$\n",
    "  P(X = x) = \\sum_{y} P(X = x, Y = y)\n",
    "  $$\n",
    "\n",
    "- **Continuous Case:**\n",
    "  \n",
    "  $$\n",
    "  P(X = x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x, y) \\, dy\n",
    "  $$\n",
    "\n",
    "This gives the probability of $ X $ occurring, regardless of $ Y $.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**  \n",
    "- **Joint probability** measures the probability of two or more events occurring together.\n",
    "- It can be computed using **conditional probability** and the **chain rule**.\n",
    "- **Independence** simplifies the computation as $ P(X, Y) = P(X) P(Y) $.\n",
    "- The **joint probability distribution (JPD)** describes how multiple variables interact.\n",
    "```\n",
    "```{admonition} Definition of Marginalization and Marginal Likelihood\n",
    ":class: note, dropdown\n",
    "**Marginalization** and **marginal likelihood** are related concepts in probability and Bayesian inference, both involving summing or integrating over hidden or unobserved variables. However, they serve different purposes.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Marginalization**  \n",
    "\n",
    "Marginalization refers to the process of obtaining the probability of a **subset** of random variables by summing or integrating over the remaining variables.\n",
    "\n",
    "- **For discrete random variables**, marginalization is done by summing over all possible values of another variable:\n",
    "\n",
    "  $$\n",
    "  P(X = x) = \\sum_{y} P(X = x, Y = y)\n",
    "  $$\n",
    "\n",
    "- **For continuous random variables**, marginalization is done by integrating over the unwanted variable:\n",
    "\n",
    "  $$\n",
    "  P(X = x) = \\int_{-\\infty}^{\\infty} P(X = x, Y = y) \\, dy\n",
    "  $$\n",
    "\n",
    "This process removes the dependency on the second variable, leaving only the probability distribution for the first variable.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Marginalization in a Joint Distribution**  \n",
    "\n",
    "Consider a **joint probability table** for two discrete variables $ X $ and $ Y $:\n",
    "\n",
    "| $ X \\backslash Y $ | $ Y = 0 $ | $ Y = 1 $ | Marginal $ P(X) $ |\n",
    "|------------------|---------|---------|--------------|\n",
    "| $ X = 0 $ | $ 0.2 $ | $ 0.3 $ | $ 0.2 + 0.3 = 0.5 $ |\n",
    "| $ X = 1 $ | $ 0.1 $ | $ 0.4 $ | $ 0.1 + 0.4 = 0.5 $ |\n",
    "\n",
    "The **marginal probability** of $ X = 0 $ is:\n",
    "\n",
    "$$\n",
    "P(X = 0) = P(X = 0, Y = 0) + P(X = 0, Y = 1) = 0.2 + 0.3 = 0.5\n",
    "$$\n",
    "\n",
    "This removes the dependency on $ Y $, leaving only the probabilities for $ X $.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Marginal Likelihood (Evidence in Bayesian Inference)**  \n",
    "\n",
    "The **marginal likelihood**, also called the **evidence**, is the probability of observed data, **integrating out any hidden or latent variables**.\n",
    "\n",
    "If $ X $ represents the observed data and $ Z $ is a latent (hidden) variable, the marginal likelihood is:\n",
    "\n",
    "- **For discrete variables**:\n",
    "\n",
    "  $$\n",
    "  P(X) = \\sum_{Z} P(X \\mid Z) P(Z)\n",
    "  $$\n",
    "\n",
    "- **For continuous variables**:\n",
    "\n",
    "  $$\n",
    "  P(X) = \\int P(X \\mid Z) P(Z) \\, dZ\n",
    "  $$\n",
    "\n",
    "This integral sums over all possible values of the latent variable $ Z $, making $ P(X) $ a **weighted sum of likelihoods over all possible latent variables**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Bayesian Model Evidence**  \n",
    "\n",
    "Suppose we are classifying an email as **spam ($ S $) or not spam ($ \\neg S $)**, but we do not know the exact proportion of spam emails. Let:\n",
    "\n",
    "- $ X $ = \"email contains the word 'free'\"\n",
    "- $ S $ = \"email is spam\"\n",
    "- $ P(X \\mid S) = 0.8 $ (80% of spam emails contain \"free\")\n",
    "- $ P(X \\mid \\neg S) = 0.1 $ (only 10% of non-spam emails contain \"free\")\n",
    "- $ P(S) = 0.3 $, meaning 30% of emails are spam\n",
    "- $ P(\\neg S) = 0.7 $, meaning 70% of emails are not spam\n",
    "\n",
    "Using **the law of total probability**, the marginal likelihood of $ X $ (observing \"free\") is:\n",
    "\n",
    "$$\n",
    "P(X) = P(X \\mid S) P(S) + P(X \\mid \\neg S) P(\\neg S)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(X) = (0.8 \\times 0.3) + (0.1 \\times 0.7) = 0.24 + 0.07 = 0.31\n",
    "$$\n",
    "\n",
    "This marginal likelihood helps in **Bayesian inference**, particularly in **Bayes’ theorem**:\n",
    "\n",
    "$$\n",
    "P(S \\mid X) = \\frac{P(X \\mid S) P(S)}{P(X)}\n",
    "$$\n",
    "\n",
    "which gives the probability of an email being spam given that it contains \"free.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Differences Between Marginalization and Marginal Likelihood**  \n",
    "\n",
    "| Feature | Marginalization | Marginal Likelihood |\n",
    "|---------|----------------|---------------------|\n",
    "| Definition | Computes the probability of a subset of variables by summing or integrating over the others | Computes the probability of observed data by integrating out hidden variables |\n",
    "| Purpose | To remove dependencies on other variables and find marginal probabilities | Used in **Bayesian inference** to compute the evidence for a model |\n",
    "| Formula (Discrete) | $ P(X) = \\sum_{Y} P(X, Y) $ | $ P(X) = \\sum_{Z} P(X \\mid Z) P(Z) $ |\n",
    "| Formula (Continuous) | $ P(X) = \\int P(X, Y) dy $ | $ P(X) = \\int P(X \\mid Z) P(Z) dZ $ |\n",
    "| Application | Used in probability theory, graphical models | Used in Bayesian statistics and machine learning |\n",
    "| Example | Summing over joint probabilities to get $ P(X) $ | Computing $ P(X) $ by integrating over a latent variable $ Z $ |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**  \n",
    "- **Marginalization** computes the probability of a variable by removing dependencies on other variables.\n",
    "- **Marginal probability** is found by summing (discrete case) or integrating (continuous case) over the other variables.\n",
    "- **Marginal likelihood (evidence)** is used in **Bayesian inference** to find the probability of observed data **regardless of hidden variables**.\n",
    "- These concepts are widely used in **probabilistic graphical models, Bayesian networks, and machine learning**.\n",
    "\n",
    "```\n",
    "```{admonition} What is reparametrization trick and why is it important?\n",
    ":class: note, dropdown\n",
    "\n",
    "The **reparametrization trick** is a technique used in **variational inference**, particularly in **variational autoencoders (VAEs)**, to enable gradient-based optimization of stochastic objectives. It allows gradients to pass through **random sampling operations**, making it possible to optimize models using **backpropagation**.\n",
    "\n",
    "---\n",
    "\n",
    "**1. The Problem: Why Do We Need the Reparametrization Trick?**  \n",
    "\n",
    "In **stochastic neural networks**, we often need to optimize a loss function that involves **sampling from a probability distribution**. A common scenario is optimizing the **expected value** of a function:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{z \\sim q(z \\mid x)} [f(z)]\n",
    "$$\n",
    "\n",
    "This notation represents the **expectation of a function over a probability distribution**. It means that we are computing the expectation of the function $ f(z) $ with respect to the probability distribution $ q(z \\mid x) $.\n",
    "\n",
    "- **Expectation Over a Probability Distribution:**  \n",
    "  The expectation is taken **over the distribution** $ q(z \\mid x) $, meaning that we are integrating over all possible values of $ z $ weighted by their probability under $ q(z \\mid x) $:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}_{z \\sim q(z \\mid x)} [f(z)] = \\int f(z) q(z \\mid x) dz\n",
    "  $$\n",
    "\n",
    "- **Expectation of a Function Over the Distribution:**  \n",
    "  Here, $ f(z) $ is a function of $ z $, and we want to compute its average value under the probability distribution $ q(z \\mid x) $. Since $ q(z \\mid x) $ is typically a **latent variable distribution**, we cannot compute this expectation in closed form and instead rely on **Monte Carlo sampling**.\n",
    "\n",
    "However, **direct sampling from $ q(z \\mid x) $ prevents backpropagation**, since gradients cannot flow through the sampling operation. This makes it difficult to train models that involve such expectations.\n",
    "\n",
    "---\n",
    "\n",
    "**2. The Reparametrization Trick: A Solution**  \n",
    "\n",
    "The reparametrization trick **re-writes the sampling process** in a way that allows gradients to be computed. Instead of directly sampling $ z \\sim q(z \\mid x) $, we express $ z $ as a **deterministic function** of some random noise $ \\epsilon $ and parameters $ \\mu, \\sigma $:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "This trick **separates** the randomness (introduced by $ \\epsilon $) from the learnable parameters ($ \\mu, \\sigma $), allowing **gradients to flow through $ \\mu $ and $ \\sigma $**.\n",
    "\n",
    "Now, instead of optimizing:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{z \\sim q(z \\mid x)} [f(z)]\n",
    "$$\n",
    "\n",
    "we optimize:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,1)} [f(\\mu + \\sigma \\epsilon)]\n",
    "$$\n",
    "\n",
    "which can be **differentiated w.r.t.** $ \\mu $ and $ \\sigma $ using standard **gradient-based methods**.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Example: Variational Autoencoder (VAE)**  \n",
    "\n",
    "A **Variational Autoencoder (VAE)** uses the reparametrization trick to learn a probabilistic latent representation.\n",
    "\n",
    "1. Instead of sampling directly from $ q(z \\mid x) \\sim \\mathcal{N}(\\mu, \\sigma^2) $, we sample from a standard normal distribution:\n",
    "\n",
    "   $$\n",
    "   \\epsilon \\sim \\mathcal{N}(0,1)\n",
    "   $$\n",
    "\n",
    "2. We then reparametrize:\n",
    "\n",
    "   $$\n",
    "   z = \\mu + \\sigma \\cdot \\epsilon\n",
    "   $$\n",
    "\n",
    "3. The loss function includes a **KL-divergence term** and a **reconstruction loss**, both of which require differentiability.\n",
    "\n",
    "4. The reparametrization trick allows **gradient updates to propagate through $ \\mu $ and $ \\sigma $**.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Why Is the Reparametrization Trick Important?**  \n",
    "\n",
    "✔ **Enables Backpropagation Through Stochastic Nodes**  \n",
    "   - Without this trick, gradients cannot flow through sampling operations.  \n",
    "   - It enables training probabilistic models like VAEs using **gradient descent**.\n",
    "\n",
    "✔ **Reduces Variance in Gradient Estimates**  \n",
    "   - Compared to Monte Carlo estimation methods, it provides more stable and lower-variance gradients.\n",
    "\n",
    "✔ **Used in Bayesian Deep Learning and Reinforcement Learning**  \n",
    "   - Essential for **Bayesian neural networks**, which learn uncertainty in deep learning.\n",
    "   - Applied in **policy gradients** in reinforcement learning.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Limitations and Extensions**  \n",
    "\n",
    "- **Does not work for discrete random variables**  \n",
    "  - Alternative methods like the **Gumbel-Softmax trick** are used for discrete distributions.\n",
    "\n",
    "- **Assumes reparametrizable distributions**  \n",
    "  - The trick works well for distributions like **Gaussian**, but for more complex distributions, alternative methods (e.g., normalizing flows) are needed.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Understanding the Expectation in the Given Equation**  \n",
    "\n",
    "The expectation in:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{z \\sim q(z \\mid x)} [f(z)]\n",
    "$$\n",
    "\n",
    "- **Expectation Over a Probability Distribution:**  \n",
    "  The expectation is taken over the **latent variable distribution** $ q(z \\mid x) $, meaning that we integrate over all possible values of $ z $ weighted by their probability under $ q(z \\mid x) $.\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}_{z \\sim q(z \\mid x)} [f(z)] = \\int f(z) q(z \\mid x) dz\n",
    "  $$\n",
    "\n",
    "- **Expectation of a Function Over the Distribution:**  \n",
    "  The function $ f(z) $ could represent an objective function or loss that we are optimizing. Since $ q(z \\mid x) $ is typically **complex and unknown**, we use **Monte Carlo estimation** to approximate this expectation:\n",
    "\n",
    "  $$\n",
    "  \\frac{1}{N} \\sum_{i=1}^{N} f(z_i), \\quad z_i \\sim q(z \\mid x)\n",
    "  $$\n",
    "\n",
    "  However, **direct sampling blocks gradient flow**, which is why the **reparametrization trick** is crucial.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**  \n",
    "- **Reparametrization Trick** allows **gradient-based learning** in models that involve **stochastic sampling**.  \n",
    "- Converts sampling into a **deterministic function** of noise and parameters.  \n",
    "- Used in **VAEs**, **Bayesian deep learning**, and **reinforcement learning**.  \n",
    "- **Essential for optimizing probabilistic models using backpropagation**.  \n",
    "- The expectation in $ \\mathbb{E}_{z \\sim q(z \\mid x)} [f(z)] $ is over the **distribution** $ q(z \\mid x) $, meaning we integrate over all possible values of $ z $ to compute the expected value of $ f(z) $.\n",
    "\n",
    "```\n",
    "```{admonition} What is meant by log likelihood? Why do we maximize in neural network training?\n",
    ":class: note, dropdown\n",
    "\n",
    "**1. What is Log Likelihood?**  \n",
    "\n",
    "The **log likelihood** is a fundamental concept in probability and machine learning, used to estimate model parameters by **maximizing the probability of observed data**. It is commonly applied in **maximum likelihood estimation (MLE)** and is the foundation of many loss functions in deep learning.\n",
    "\n",
    "**Likelihood Function**  \n",
    "Given a dataset $ \\mathcal{D} = \\{ x_1, x_2, \\dots, x_n \\} $, where each $ x_i $ is a data point, and a probabilistic model with parameters $ \\theta $, the **likelihood function** is defined as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = P(\\mathcal{D} \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ P(\\mathcal{D} \\mid \\theta) $ represents the probability of observing the data $ \\mathcal{D} $ given the model parameters $ \\theta $.\n",
    "- The goal of **maximum likelihood estimation (MLE)** is to find the parameters $ \\theta^* $ that maximize this probability:\n",
    "\n",
    "  $$\n",
    "  \\theta^* = \\arg\\max_{\\theta} L(\\theta)\n",
    "  $$\n",
    "\n",
    "Since models often assume **independent** data points, the likelihood function is expressed as a product:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} P(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ P(x_i \\mid \\theta) $ is the probability of the individual data point $ x_i $ under the model.\n",
    "- The product arises because we assume that each data point is **independent and identically distributed (i.i.d.)**.\n",
    "\n",
    "**Log Likelihood Function**  \n",
    "The log likelihood is simply the **logarithm** of the likelihood function:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\log P(\\mathcal{D} \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Using the i.i.d. assumption:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\log \\prod_{i=1}^{n} P(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Applying the **logarithm property** ($ \\log ab = \\log a + \\log b $):\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i=1}^{n} \\log P(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- The **logarithm transforms the product into a sum**, which is easier to compute and numerically more stable.\n",
    "- Instead of multiplying many small probabilities (which can cause numerical underflow), we sum their log probabilities.\n",
    "\n",
    "Thus, **maximizing likelihood is equivalent to maximizing the log likelihood**, which simplifies optimization.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Why Do We Maximize Log Likelihood in Neural Networks?**  \n",
    "\n",
    "Neural networks often predict probabilities. To train a model, we want to maximize the probability assigned to the correct data points, i.e., maximize:\n",
    "\n",
    "$$\n",
    "P(\\mathcal{D} \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Since working with probabilities directly can be unstable (due to small values), we use the log likelihood:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\max_{\\theta} \\log L(\\theta) = \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log P(x_i \\mid \\theta)\n",
    "$$\n",
    "\n",
    "This is the objective function used in probabilistic models.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Example: Log Likelihood in Classification (Softmax + Cross-Entropy Loss)**  \n",
    "\n",
    "In **neural network classification**, we model the probability of each class using the **softmax function**:\n",
    "\n",
    "$$\n",
    "P(y \\mid x, \\theta) = \\frac{\\exp(f_{\\theta}(x)_y)}{\\sum_{j} \\exp(f_{\\theta}(x)_j)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ f_{\\theta}(x)_y $ is the predicted score for class $ y $.\n",
    "- The denominator ensures all class probabilities sum to **1**.\n",
    "\n",
    "The likelihood for a dataset $ \\mathcal{D} $ is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} P(y_i \\mid x_i, \\theta)\n",
    "$$\n",
    "\n",
    "Taking the **log likelihood**:\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i=1}^{n} \\log P(y_i \\mid x_i, \\theta)\n",
    "$$\n",
    "\n",
    "which is **equivalent to minimizing the cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\sum_{i=1}^{n} \\log P(y_i \\mid x_i, \\theta)\n",
    "$$\n",
    "\n",
    "Thus, **maximizing log likelihood is the same as minimizing cross-entropy loss** in classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Why Log Likelihood is Used in Training?**  \n",
    "\n",
    "✔ **Converts Products into Sums**  \n",
    "   - Probabilities are small values between **0 and 1**.\n",
    "   - Multiplying many probabilities leads to **numerical underflow**.\n",
    "   - Taking the **log** prevents this issue by converting the product into a sum.\n",
    "\n",
    "✔ **Easier Optimization**  \n",
    "   - Log likelihood often results in **convex** loss functions, making gradient-based optimization more effective.\n",
    "\n",
    "✔ **Directly Related to Cross-Entropy Loss**  \n",
    "   - In classification, **maximizing log likelihood = minimizing cross-entropy loss**.\n",
    "\n",
    "✔ **Has a Probabilistic Interpretation**  \n",
    "   - Maximizing log likelihood ensures our model assigns **high probability to observed data**, leading to better generalization.\n",
    "\n",
    "✔ **Log Likelihood Gradient Helps in Backpropagation**  \n",
    "   - The gradients of log likelihood are well-defined, ensuring smooth updates in gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Key Takeaways**  \n",
    "\n",
    "- **Log likelihood** is the logarithm of the likelihood function, used to estimate parameters in probabilistic models.  \n",
    "- **Maximizing log likelihood** finds the best parameters that make the observed data most probable.  \n",
    "- **In neural networks**, log likelihood optimization is equivalent to minimizing **cross-entropy loss** for classification.  \n",
    "- **Computationally stable** and helps avoid underflow in probability calculations.  \n",
    "- **Essential in probabilistic deep learning models**, such as **VAEs, Bayesian neural networks, and language models**.\n",
    "\n",
    "```\n",
    "```{admonition} What is KL Divergence and why is it important?\n",
    ":class: note, dropdown\n",
    "\n",
    "**1. What is KL Divergence?**  \n",
    "\n",
    "**Kullback-Leibler (KL) Divergence** is a fundamental concept in probability theory and machine learning. It measures how one probability distribution differs from another. In other words, it quantifies the **information loss** when we approximate a true distribution with another.\n",
    "\n",
    "For two probability distributions:\n",
    "- **True distribution**: $ P(x) $\n",
    "- **Approximate distribution**: $ Q(x) $\n",
    "\n",
    "The **KL divergence** is defined as:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)} \\quad \\text{(discrete case)}\n",
    "$$\n",
    "\n",
    "or, for continuous distributions:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\int P(x) \\log \\frac{P(x)}{Q(x)} \\, dx\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ P(x) $ is the **true** distribution (e.g., the actual data distribution).\n",
    "- $ Q(x) $ is the **approximate** distribution (e.g., a model trying to approximate $ P $).\n",
    "- The **log ratio** measures how much $ P(x) $ and $ Q(x) $ diverge at each point.\n",
    "\n",
    "The **KL divergence is always non-negative**:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) \\geq 0\n",
    "$$\n",
    "\n",
    "with equality ($ D_{\\text{KL}}(P \\parallel Q) = 0 $) if and only if **$ P(x) = Q(x) $ for all $ x $**.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Intuition Behind KL Divergence**  \n",
    "\n",
    "✔ **Measures Information Loss**  \n",
    "   - If we use $ Q(x) $ to approximate $ P(x) $, KL divergence tells us **how much information is lost**.\n",
    "\n",
    "✔ **Asymmetry: $ D_{\\text{KL}}(P \\parallel Q) \\neq D_{\\text{KL}}(Q \\parallel P) $**  \n",
    "   - KL divergence is **not symmetric**, meaning it is **not a true distance metric**.\n",
    "\n",
    "✔ **Expectation of Log Difference**  \n",
    "   - The term $ \\log \\frac{P(x)}{Q(x)} $ represents the log difference between the two distributions.\n",
    "   - KL divergence takes the **expectation under $ P(x) $**, meaning that the true distribution **weights the difference**.\n",
    "\n",
    "✔ **Lower KL = Better Approximation**  \n",
    "   - If $ D_{\\text{KL}}(P \\parallel Q) $ is small, $ Q(x) $ is a good approximation of $ P(x) $.\n",
    "   - If KL is large, $ Q(x) $ is far from $ P(x) $, meaning a poor approximation.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Why is KL Divergence Important?**  \n",
    "\n",
    "KL divergence is widely used in **machine learning, statistics, and deep learning** for **probability estimation, model optimization, and generative modeling**.\n",
    "\n",
    "✔ **Used in Variational Inference**  \n",
    "   - In **variational autoencoders (VAEs)**, KL divergence is used to **regularize** the latent space by forcing the approximate posterior $ Q(z \\mid x) $ to be close to a prior $ P(z) $.\n",
    "\n",
    "✔ **Used in Bayesian Deep Learning**  \n",
    "   - KL divergence measures how much information is lost when using an **approximate posterior** instead of the **true Bayesian posterior**.\n",
    "\n",
    "✔ **Used in Reinforcement Learning (RL)**  \n",
    "   - In **policy optimization**, KL divergence ensures that updates do not drastically change the policy distribution.\n",
    "\n",
    "✔ **Related to Cross-Entropy Loss**  \n",
    "   - Cross-entropy loss in classification problems is directly related to KL divergence.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Example: KL Divergence Between Two Normal Distributions**  \n",
    "\n",
    "For two Gaussian distributions:\n",
    "\n",
    "- **True distribution**: $ P(x) = \\mathcal{N}(\\mu_1, \\sigma_1^2) $\n",
    "- **Approximate distribution**: $ Q(x) = \\mathcal{N}(\\mu_2, \\sigma_2^2) $\n",
    "\n",
    "The KL divergence is:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) =\n",
    "\\log \\frac{\\sigma_2}{\\sigma_1} +\n",
    "\\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The **first term** measures the difference in **variance**.\n",
    "- The **second term** measures the difference in **mean**.\n",
    "- If $ \\mu_1 = \\mu_2 $ and $ \\sigma_1 = \\sigma_2 $, then $ D_{\\text{KL}}(P \\parallel Q) = 0 $.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Symmetric Alternative: Jensen-Shannon (JS) Divergence**  \n",
    "\n",
    "Since KL divergence is **not symmetric**, an alternative is **Jensen-Shannon divergence (JS divergence)**:\n",
    "\n",
    "$$\n",
    "D_{\\text{JS}}(P \\parallel Q) = \\frac{1}{2} D_{\\text{KL}}(P \\parallel M) + \\frac{1}{2} D_{\\text{KL}}(Q \\parallel M)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "M(x) = \\frac{1}{2} (P(x) + Q(x))\n",
    "$$\n",
    "\n",
    "JS divergence **is symmetric** and **bounded between 0 and 1**, making it useful for comparing distributions.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Key Takeaways**  \n",
    "\n",
    "✔ **KL divergence measures the difference between two probability distributions.**  \n",
    "✔ **It quantifies how much information is lost when using $ Q(x) $ instead of $ P(x) $.**  \n",
    "✔ **Lower KL means a better approximation.**  \n",
    "✔ **Used in VAEs, Bayesian deep learning, RL, and probability models.**  \n",
    "✔ **Not symmetric: $ D_{\\text{KL}}(P \\parallel Q) \\neq D_{\\text{KL}}(Q \\parallel P) $.**  \n",
    "✔ **JS divergence is a symmetric alternative.**\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "732d8975-53f7-4f52-8ff6-d789a8d6b24d",
   "metadata": {},
   "source": [
    "## Forward Process\n",
    "\n",
    "![image.png](figures/gaussian_diff.png)\n",
    "\n",
    "```{admonition} How does joint distribution and marginal distribution come into picture here?\n",
    ":class: note, dropdown\n",
    "\n",
    "**Understanding Equation (1) in the Context of Joint and Marginal Distributions**  \n",
    "\n",
    "1. **Understanding Equation (1) and the Joint Distribution**  \n",
    "\n",
    "   The given equation describes a **stochastic process** where noise is added to a random variable iteratively:\n",
    "\n",
    "   $$\n",
    "   x_{t+1} := x_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma^2).\n",
    "   $$\n",
    "\n",
    "   Here:\n",
    "   - $ x_0 $ is the **original data** drawn from the true data distribution $ p^*(x_0) $.\n",
    "   - $ x_{t+1} $ is generated **recursively** from $ x_t $ by adding Gaussian noise $ \\eta_t $.\n",
    "   - $ \\eta_t $ follows a **normal distribution** $ \\mathcal{N}(0, \\sigma^2) $.\n",
    "   - $ \\sigma^2 $ controls the scale of noise added at each step.\n",
    "\n",
    "   This **iterative noise addition** results in a **joint distribution** over the entire sequence:\n",
    "\n",
    "   $$\n",
    "   p(x_0, x_1, \\dots, x_T).\n",
    "   $$\n",
    "\n",
    "   **Why Does This Define a Joint Distribution?**  \n",
    "   A **joint distribution** describes the probability of all random variables occurring together. The equation defines a **Markov chain**, where:\n",
    "\n",
    "   $$\n",
    "   p(x_0, x_1, \\dots, x_T) = p(x_0) \\prod_{t=1}^{T} p(x_t \\mid x_{t-1}).\n",
    "   $$\n",
    "\n",
    "   Each transition follows a **Gaussian conditional probability**:\n",
    "\n",
    "   $$\n",
    "   p(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t \\mid x_{t-1}, \\sigma^2).\n",
    "   $$\n",
    "\n",
    "   This means:\n",
    "   - The probability of a full trajectory $ (x_0, x_1, ..., x_T) $ is determined by the **initial distribution** $ p(x_0) $ and the **conditional distributions** $ p(x_t \\mid x_{t-1}) $.\n",
    "   - Since each $ x_t $ is conditionally dependent only on $ x_{t-1} $, this process follows a **Markov property**.\n",
    "   - Expanding one transition step mathematically:\n",
    "\n",
    "     $$\n",
    "     x_t = x_{t-1} + \\eta_{t-1}, \\quad \\text{where } \\eta_{t-1} \\sim \\mathcal{N}(0, \\sigma^2).\n",
    "     $$\n",
    "\n",
    "     This shows that $ x_t $ is sampled from a Gaussian distribution centered at $ x_{t-1} $ with variance $ \\sigma^2 $.\n",
    "\n",
    "2. **How the Marginal Distribution Appears in This Process**  \n",
    "\n",
    "   A **marginal distribution** is obtained by summing (discrete case) or integrating (continuous case) over unwanted variables in a joint distribution.\n",
    "\n",
    "   To find the **marginal distribution** of $ x_t $, we integrate out previous states:\n",
    "\n",
    "   $$\n",
    "   p_t(x_t) = \\int p(x_t \\mid x_{t-1}) p_{t-1}(x_{t-1}) \\, dx_{t-1}.\n",
    "   $$\n",
    "\n",
    "   By repeating this process, we marginalize out all prior steps:\n",
    "\n",
    "   $$\n",
    "   p_t(x_t) = \\int p(x_t \\mid x_{t-1}) p(x_{t-1} \\mid x_{t-2}) \\dots p(x_1 \\mid x_0) p(x_0) \\, dx_0 \\dots dx_{t-1}.\n",
    "   $$\n",
    "\n",
    "   This equation shows that the marginal distribution $ p_t(x_t) $ depends on how noise **accumulates** over time.\n",
    "\n",
    "3. **What Happens as $ T \\to \\infty $?**  \n",
    "\n",
    "   As $ t $ increases, more noise is added, and the **marginal distribution** $ p_T(x_T) $ approaches a **Gaussian distribution**, regardless of the original data distribution $ p^*(x_0) $:\n",
    "\n",
    "   $$\n",
    "   p_T(x_T) \\approx \\mathcal{N}(0, \\sigma^2 I).\n",
    "   $$\n",
    "\n",
    "   This means that after **many diffusion steps**, the data is **completely transformed into Gaussian noise**, losing its original structure.\n",
    "\n",
    "4. **Summary of Key Points**  \n",
    "\n",
    "   ✔ **Equation (1) defines a joint distribution** because it constructs a **probabilistic chain** over multiple variables with **conditional dependencies**.  \n",
    "   ✔ **The marginal distribution** of each $ x_t $ is obtained by integrating out previous steps from the joint distribution.  \n",
    "   ✔ As the process progresses, the **marginal distribution of $ x_T $ converges to a Gaussian**, meaning the data is transformed into pure noise.  \n",
    "\n",
    "```\n",
    "\n",
    "## Reverse Process\n",
    "\n",
    "We have a way to start with a data point $x_0$ and reach the point $x_t$ following the forward diffusion noise addition. Now lets, use this information to formulate a problem statement to represent the reverse diffusion process.\n",
    "\n",
    "![image.png](figures/rev_diff.png)\n",
    "\n",
    "```{admonition} More details on the reverse process\n",
    ":class: note, dropdown\n",
    "1. **Understanding the Forward and Reverse Diffusion Process**  \n",
    "\n",
    "   The **forward diffusion process** follows the equation:\n",
    "\n",
    "   $$\n",
    "   x_t = x_{t-1} + \\sqrt{\\beta_t} \\eta, \\quad \\eta \\sim \\mathcal{N}(0, I).\n",
    "   $$\n",
    "\n",
    "   - Here, **$ \\beta_t $ is a time-dependent noise variance**, meaning the **variance changes at each step** but the **mean remains dependent on $ x_{t-1} $**.\n",
    "   - The mean of this transition process is simply **$ x_{t-1} $**, implying that at each step, we retain information about the previous state but add Gaussian noise.\n",
    "\n",
    "   In contrast, the **reverse diffusion process** aims to recover $ x_{t-1} $ given $ x_t $, using:\n",
    "\n",
    "   $$\n",
    "   p(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1} \\mid \\mu_{\\theta}(x_t), \\sigma_t^2 I).\n",
    "   $$\n",
    "\n",
    "   - The **mean function $\\mu_{\\theta}(x_t)$ is learned** and does not remain constant.\n",
    "   - The **variance $\\sigma_t^2$ can be learned or fixed**, depending on the formulation.\n",
    "\n",
    "2. **What Happens When the Reverse Process is Not Gaussian?**  \n",
    "\n",
    "   If the conditional probability **$ p(x_{t-1} \\mid x_t) $ is not Gaussian**, then we cannot directly sample from a simple normal distribution. Instead, we must compute:\n",
    "\n",
    "   $$\n",
    "   p(x_{t-1} \\mid x_t) = \\frac{p(x_t \\mid x_{t-1}) p(x_{t-1})}{p(x_t)}.\n",
    "   $$\n",
    "\n",
    "   - **$ p(x_t \\mid x_{t-1}) $** is the known forward transition (Gaussian).\n",
    "   - **$ p(x_t) $** is the marginal distribution obtained by integrating over all possible previous states:\n",
    "\n",
    "     $$\n",
    "     p(x_t) = \\int p(x_t \\mid x_{t-1}) p(x_{t-1}) \\, dx_{t-1}.\n",
    "     $$\n",
    "\n",
    "   - This requires solving an **intractable integral** over all possible prior states, making direct computation difficult.\n",
    "\n",
    "3. **Equation to be Solved for Reverse Diffusion**  \n",
    "\n",
    "   Since direct computation of **$ p(x_{t-1} \\mid x_t) $** is intractable, we need to approximate it. The equation to solve in the general case is:\n",
    "\n",
    "   $$\n",
    "   p(x_{t-1} \\mid x_t) \\propto p(x_t \\mid x_{t-1}) p(x_{t-1}).\n",
    "   $$\n",
    "\n",
    "   - If **$ p(x_{t-1}) $** is not Gaussian, this distribution becomes **complex and multimodal**, requiring advanced generative modeling techniques like **normalizing flows or energy-based models**.\n",
    "   - In such cases, sampling from **$ p(x_{t-1} \\mid x_t) $** is difficult because we do not have a closed-form solution.\n",
    "\n",
    "4. **How the Reverse Process Simplifies If It Is Gaussian**  \n",
    "\n",
    "   If we assume that **$ p(x_{t-1}) $ is also Gaussian**, then **Bayesian inference simplifies the conditional probability** into another Gaussian:\n",
    "\n",
    "   $$\n",
    "   p(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1} \\mid \\mu_{\\theta}(x_t), \\sigma_t^2 I).\n",
    "   $$\n",
    "\n",
    "   - This means the entire reverse process can be modeled using a **deep neural network** that predicts the **mean function $\\mu_{\\theta}(x_t)$**.\n",
    "   - The **variance $\\sigma_t^2$ can either be learned or set using a predefined noise schedule**.\n",
    "\n",
    "5. **Key Takeaways**  \n",
    "\n",
    "   ✔ **The forward process has a fixed mean at each step ($ x_{t-1} $), but the variance increases with time ($ \\beta_t $).**  \n",
    "   ✔ **The reverse process does not have a fixed mean; instead, it must be learned using a function $ \\mu_{\\theta}(x_t) $.**  \n",
    "   ✔ **If the reverse process is non-Gaussian, we must marginalize over all previous states, requiring intractable integrations.**  \n",
    "   ✔ **If the reverse process is Gaussian, it simplifies to a normal distribution with a learnable mean and variance, making training feasible.**    \n",
    "```\n",
    "\n",
    "![](figures/small_sigma_more_gauss_rev.png)\n",
    "\n",
    "![](figures/rev_diff1.png)\n",
    "\n",
    "The goal of the reverse process is to recover **$ x_{t-1} $** from **$ x_t $**. However, directly modeling the full conditional probability **$ p(x_{t-1} \\mid x_t) $** can be difficult. Instead, we focus on learning the **mean** of this distribution, denoted as **$ \\mu_{t-1}(x_t) $**, since for a **Gaussian assumption**, knowing the mean is sufficient to describe the distribution.\n",
    "\n",
    "Thus, instead of learning the entire probability distribution **$ p(x_{t-1} \\mid x_t) $**, we approximate it using the expectation:\n",
    "\n",
    "$$\n",
    "\\mu_{t-1}(z) := \\mathbb{E}[x_{t-1} \\mid x_t = z]\n",
    "$$\n",
    "\n",
    "This means that given **a specific value of $ x_t $**, the best estimate of $ x_{t-1} $ is its expected value.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Why Are We Taking an Expectation?**  \n",
    "\n",
    "Expectation appears because in the **reverse diffusion process**, each step is **stochastic**, meaning multiple values of **$ x_{t-1} $** could lead to the same $ x_t $ due to added noise in the forward process.\n",
    "\n",
    "- The expectation **$ \\mathbb{E}[x_{t-1} \\mid x_t] $** gives us the **most likely** previous step given the current state $ x_t $.\n",
    "- Instead of predicting a single deterministic value, we predict **the expected value** of $ x_{t-1} $.\n",
    "\n",
    "Since the forward process follows:\n",
    "\n",
    "$$\n",
    "x_t = x_{t-1} + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "taking expectation removes the randomness introduced by $ \\eta_t $ and gives the best estimate of $ x_{t-1} $.\n",
    "\n",
    "---\n",
    "\n",
    "**3. How Do We Estimate the Expectation? (Regression Formulation)**  \n",
    "\n",
    "To estimate **$ \\mu_{t-1}(x_t) $**, we approximate it using a **function $ f(x_t) $** that tries to **predict** $ x_{t-1} $. This function is found by minimizing the **mean squared error** (MSE) loss:\n",
    "\n",
    "$$\n",
    "\\mu_{t-1} = \\arg\\min_{f: \\mathbb{R}^d \\to \\mathbb{R}^d} \\mathbb{E}_{x_t, x_{t-1}} ||f(x_t) - x_{t-1}||_2^2.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- **$ f(x_t) $** is a neural network (or any regression model) that tries to predict $ x_{t-1} $.\n",
    "- **The squared norm $ ||f(x_t) - x_{t-1}||_2^2 $** measures how far our prediction is from the true value.\n",
    "- **Expectation $ \\mathbb{E}_{x_t, x_{t-1}} $** means we average over many samples to find the best function $ f(x_t) $ that minimizes this error.\n",
    "\n",
    "This is a standard **regression problem**: we are training a function to predict **$ x_{t-1} $** given **$ x_t $** by minimizing the squared error.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Why is the Expectation Over $ x_t, x_{t-1} $?**  \n",
    "\n",
    "- **We do not have a single $ x_t $ for each $ x_{t-1} $** due to the randomness in the forward process.\n",
    "- Instead, multiple **$ x_{t-1} $** values contribute to the observed $ x_t $ because of the Gaussian noise **$ \\eta $**.\n",
    "- This means we must train our function **by averaging over all possible ($ x_t, x_{t-1} $) pairs** from the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Alternative Expression Using the Forward Process**  \n",
    "\n",
    "Since we know the forward step follows:\n",
    "\n",
    "$$\n",
    "x_t = x_{t-1} + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "we can rewrite the regression formulation as:\n",
    "\n",
    "$$\n",
    "\\mu_{t-1} = \\arg\\min_{f: \\mathbb{R}^d \\to \\mathbb{R}^d} \\mathbb{E}_{x_{t-1}, \\eta} ||f(x_{t-1} + \\eta_t) - x_{t-1}||_2^2.\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- Instead of regressing directly on ($ x_t, x_{t-1} $) pairs, we use the known **noisy forward process** to **generate** training pairs.\n",
    "- Given **$ x_{t-1} $**, we add Gaussian noise **$ \\eta_t $** to obtain **$ x_t $**, and then train our model **to recover $ x_{t-1} $ from $ x_t $**\n",
    "\n",
    "This regression **maps noisy inputs to clean ones**, making it essentially a **denoising problem**.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Connection to Image Denoising and Deep Learning**  \n",
    "\n",
    "- If **$ p^* $** is an image distribution, the regression objective becomes:\n",
    "  - Given a **noisy image $ x_t $**, predict the **clean image $ x_{t-1} $**.\n",
    "  - This is exactly what **image denoising networks** do, explaining why diffusion models can be trained using **convolutional neural networks (CNNs)**.\n",
    "\n",
    "- The function **$ f(x_t) $** is usually parameterized by a neural network:\n",
    "  - **UNet architectures** are commonly used because they are good at removing noise at different scales.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Summary of Key Concepts**  \n",
    "\n",
    "**Instead of modeling the full conditional distribution $ p(x_{t-1} \\mid x_t) $, we only learn the mean function $ \\mathbb{E}[x_{t-1} \\mid x_t] $**, which simplifies the problem.  \n",
    "**Expectation is necessary because multiple $ x_{t-1} $ values could lead to the same $ x_t $ due to noise, and we need the best average estimate.**  \n",
    "**We solve this as a regression problem: given $ x_t $, predict $ x_{t-1} $ using a function $ f(x_t) $, trained to minimize mean squared error (MSE).**  \n",
    "**Instead of using real ($ x_t, x_{t-1} $) pairs, we simulate them using the known forward process by adding Gaussian noise.**  \n",
    "**This makes diffusion models closely related to denoising models, which explains why CNNs work well in this context.**  \n",
    "\n",
    "```{admonition} Independent and Identically Distributed\n",
    ":class: note, dropdown\n",
    "\n",
    "1. **Definition of i.i.d.**  \n",
    "\n",
    "   The term **i.i.d.** stands for **Independent and Identically Distributed** and refers to a collection of random variables that satisfy two conditions:\n",
    "\n",
    "   - **Independent:** Each random variable does not affect the others.\n",
    "   - **Identically Distributed:** All random variables follow the same probability distribution.\n",
    "\n",
    "   Mathematically, a sequence of random variables **$ X_1, X_2, ..., X_n $** is **i.i.d.** if:\n",
    "\n",
    "   - **Independence:**  \n",
    "   \n",
    "     $$\n",
    "     P(X_1, X_2, ..., X_n) = P(X_1) P(X_2) ... P(X_n).\n",
    "     $$\n",
    "     \n",
    "     This means knowing the value of **$ X_1 $** does not provide any information about **$ X_2 $**, and so on.\n",
    "\n",
    "   - **Identical Distribution:**  \n",
    "     For all **$ i $**, the probability distribution of **$ X_i $** is the same:\n",
    "     \n",
    "     $$\n",
    "     P(X_i \\leq x) = P(X_j \\leq x), \\quad \\forall i, j.\n",
    "     $$\n",
    "     \n",
    "     This means all random variables are drawn from the same probability distribution.\n",
    "\n",
    "2. **Why is i.i.d. Important?**  \n",
    "\n",
    "   Many fundamental results in probability, statistics, and machine learning assume that data points are **i.i.d.** because it simplifies analysis and model training.\n",
    "\n",
    "   ✔ **Allows Use of the Law of Large Numbers (LLN):**  \n",
    "      - If we take many i.i.d. samples, the sample mean **converges to the true mean** of the distribution.\n",
    "\n",
    "   ✔ **Enables the Central Limit Theorem (CLT):**  \n",
    "      - The sum (or average) of i.i.d. random variables follows a **normal distribution** when the number of samples is large, regardless of the original distribution.\n",
    "\n",
    "   ✔ **Simplifies Machine Learning Models:**  \n",
    "      - Many algorithms assume that training samples are **independent and identically distributed** to avoid biases in learning patterns.\n",
    "\n",
    "   ✔ **Makes Statistical Inference Easier:**  \n",
    "      - i.i.d. samples allow estimators (like mean and variance) to be **unbiased and consistent**.\n",
    "\n",
    "3. **Examples of i.i.d. and Non-i.i.d. Data**  \n",
    "\n",
    "   - **i.i.d. Example:**  \n",
    "     - Rolling a fair six-sided die multiple times.\n",
    "     - Each roll does not affect the next roll (independence).\n",
    "     - Each roll has the same probability distribution (identical distribution).\n",
    "\n",
    "   - **Non-i.i.d. Example (Dependent Data):**  \n",
    "     - Stock prices: The price today depends on yesterday’s price.\n",
    "     - Sentences in a book: The probability of a word depends on the previous words (language models assume sequential dependence).\n",
    "     - Weather conditions: Tomorrow’s temperature depends on today’s temperature.\n",
    "\n",
    "4. **Mathematical Implications of i.i.d. in Machine Learning**  \n",
    "\n",
    "   - Suppose we have a dataset **$ X_1, X_2, ..., X_n $** that is i.i.d. with mean **$ \\mu $** and variance **$ \\sigma^2 $**.\n",
    "   - The **sample mean** is:\n",
    "   \n",
    "     $$\n",
    "     \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i.\n",
    "     $$\n",
    "     \n",
    "   - The **Law of Large Numbers (LLN)** states:\n",
    "   \n",
    "     $$\n",
    "     \\bar{X} \\to \\mu \\quad \\text{as } n \\to \\infty.\n",
    "     $$\n",
    "     \n",
    "     This ensures that as we collect more data, our estimates become more accurate.\n",
    "\n",
    "   - The **Central Limit Theorem (CLT)** states:\n",
    "   \n",
    "     $$\n",
    "     \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\approx \\mathcal{N}(0,1) \\quad \\text{for large } n.\n",
    "     $$\n",
    "     \n",
    "     This means that even if the original data distribution is not normal, the sample mean follows a **normal distribution**.\n",
    "\n",
    "5. **Summary of Key Takeaways**  \n",
    "\n",
    "   ✔ **i.i.d. means that each data point is drawn independently from the same probability distribution.**  \n",
    "   ✔ **It simplifies many statistical and machine learning models, making inference easier.**  \n",
    "   ✔ **Key theorems like LLN and CLT rely on i.i.d. assumptions for convergence properties.**  \n",
    "   ✔ **Real-world data is often not strictly i.i.d. (e.g., time series, text, stock prices), requiring specialized models.**  \n",
    "```\n",
    "\n",
    "## Discretization of probability distribution\n",
    "\n",
    "The key idea here is that we model the evolution of a probability distribution **continuously in time** rather than using discrete jumps. We want to understand how the distributions at each step are related and ensure that the final distribution is **independent of the choice of discretization steps**.\n",
    "\n",
    "---\n",
    "\n",
    "**1. The Meaning of Discretization in Diffusion**  \n",
    "\n",
    "We define a sequence of probability distributions **$ p_0, p_1, \\dots, p_T $** as a **discretization** of a **continuous-time function** $ p(x,t) $, where:\n",
    "\n",
    "- **$ p_0(x) $** is the **original data distribution**.\n",
    "- **$ p_T(x) $** is the **final noisy distribution** (Gaussian).\n",
    "- **$ p(x, t) $** evolves continuously over time from **$ t=0 $** to **$ t=1 $**.\n",
    "\n",
    "Instead of using discrete steps, we describe this as a **smooth, continuous-time process**:\n",
    "\n",
    "$$\n",
    "p(x, k\\Delta t) = p_k(x), \\quad \\text{where} \\quad \\Delta t = \\frac{1}{T}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**2. Why Is This Important?**\n",
    "- **The number of steps $T$ controls how fine the discretization is**:\n",
    "  - If **$T$ is large**, adjacent distributions **$p_t$** and **$p_{t-1}$** are **very close**.\n",
    "  - If **$T$ is small**, adjacent distributions are **more different**, making the reverse process harder.\n",
    "- **This leads naturally to a continuous-time description of diffusion**, which connects to **stochastic differential equations (SDEs)**.\n",
    "\n",
    "---\n",
    "\n",
    "**Variance Scaling and the Noise Process**\n",
    "\n",
    "**3. Understanding the Variance at Each Step**\n",
    "The forward diffusion process at each discrete step is defined as:\n",
    "\n",
    "$$\n",
    "x_k = x_{k-1} + \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Since noise is added at every step, the distribution of $x_T$ after $T$ steps is:\n",
    "\n",
    "$$\n",
    "x_T \\sim \\mathcal{N}(x_0, T\\sigma^2)\n",
    "$$\n",
    "\n",
    "This means that the **total variance grows linearly** with the number of steps **$T$**. However, we want the final variance to be **fixed and independent of $T$**.\n",
    "\n",
    "To achieve this, we **scale the noise variance** at each step using:\n",
    "\n",
    "$$\n",
    "\\sigma = \\sigma_q \\sqrt{\\Delta t}, \\quad \\text{where} \\quad \\Delta t = \\frac{1}{T}\n",
    "$$\n",
    "\n",
    "This ensures that:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{T} \\sigma^2 = \\sigma_q^2\n",
    "$$\n",
    "\n",
    "Thus, the **total variance remains constant** regardless of the number of steps **$T$**.\n",
    "\n",
    "---\n",
    "\n",
    "**4. The Final Continuous-Time Formulation**\n",
    "Now, we switch from **discrete indexing** $ k $ to a **continuous-time variable** $ t \\in [0,1] $.\n",
    "\n",
    "- Instead of writing **$ x_k $**, we use **$ x_t $** to represent the state of **$ x $** at time **$ t $**.\n",
    "- The forward process becomes:\n",
    "\n",
    "$$\n",
    "x_{t+\\Delta t} = x_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_q^2 \\Delta t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**5. What This Implies for the Marginal Distribution**\n",
    "Since noise accumulates over time, the **total noise added up to time $ t $** is:\n",
    "\n",
    "$$\n",
    "x_t \\sim \\mathcal{N}(x_0, \\sigma_t^2), \\quad \\text{where} \\quad \\sigma_t = \\sigma_q \\sqrt{t}\n",
    "$$\n",
    "\n",
    "This ensures that the process remains **Gaussian at all times**, which is crucial for the **reverse process**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Mathematical Insights**\n",
    "\n",
    "**6. Why Scaling the Variance is Necessary**\n",
    "If we did **not** scale the variance correctly:\n",
    "- The **total variance would depend on $ T $**, meaning the final distribution **$ p_T(x) $** would change if we modified the number of steps.\n",
    "- This would make **training unstable**, as different choices of **$T$** would lead to different results.\n",
    "\n",
    "By choosing:\n",
    "\n",
    "$$\n",
    "\\sigma = \\sigma_q \\sqrt{\\Delta t}\n",
    "$$\n",
    "\n",
    "we ensure that the **final variance is always $\\sigma_q^2$**, making the diffusion process **consistent**.\n",
    "\n",
    "---\n",
    "\n",
    "**7. The Role of Continuous-Time Notation**\n",
    "- The switch from **discrete indices** $ k $ to a **continuous variable** $ t $ makes the transition smoother.\n",
    "- We can now describe the process using **differential equations**, which makes analysis and computation easier.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Everything We Covered**\n",
    "\n",
    "✔ **The probability distributions $ p_0, …, p_T $ are a discrete approximation of a continuous function $ p(x,t) $.**  \n",
    "✔ **The variance must be scaled properly to ensure that the final distribution does not depend on the number of steps $T$.**  \n",
    "✔ **Switching to a continuous-time formulation allows us to use stochastic differential equations (SDEs).**  \n",
    "✔ **The total noise added over time follows a Gaussian distribution with a variance that grows proportionally to time.**  \n",
    "✔ **The final step is to express the forward and reverse processes in terms of SDEs, which is what we will explore next.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9b7c3-d2a5-41a7-9fb1-5c1eef5a3bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
